---
title: 'Real Estate Evaluation'
author: "Kevin Sandoval"
date: "6/9/2022"
output: pdf_document
---
\raggedright
\clearpage
\tableofcontents
\newpage
```{r, include=FALSE}
load('FinalProject.RData')
```

#Abstract

In this project, I examined real estate valuation from Sindian District, New Taipei City, Taiwan and looked to see how different predictors and different transformations would affect the price of real estate in this city in Taiwan. I found that the best regression I could do for price was TDate, sqrt(Age), log(Metro), and Latitude as my predictors. I also examined the eight different components of concrete strength, to determine which components were the most significant as predictors. I found that Cement(X1) , Blast Furnace Slag(X2), Fly Ash(X3), Water(X4), Superplasticizer(X5), and Age(X8) were the most significant predictors in determining concrete strength. 

#Problem and Motivation

For the first data set, the information provided is relevant because there are many people who may be impacted by the findings. For example, if someone in Taiwan wanted to purchase real estate in Taiwan, the data I collected and compiled can give an insight into what would cause this person to pay more, or less money for their real estate. In my testing, I found in my best regression model that for every unit (year) that TDate increases, each square meter in the house will increase in price by 1,794 Taiwanese dollars. This is interesting as this predictor on its own, implies that waiting to buy will cost you money. The concrete dataset is a similar representation. The strength of concrete is an integral part of the society I live in today, as sidewalks, roads, and bridges are often made using this material. It is important to distinguish the important factors that contribute to the overall effect of the strength of concrete. As stated in the abstract, I found that certain predictors play a large role in determining the overall strength of the concrete, which would impact which parts of the construction process should be more heavily focused on and considered.

#Description of Data

The real estate valuation data looks at different predictors and their effect on the price of the houses in that real estate market. In my scenario, the relevant variables that I have are TDate (transaction date), Age, Metro, Latitude. In the Concrete dataset, I examined the predictors effects on the strength of the concrete as a whole. I found are notable variables to be Cement(X1) , Blast Furnace Slag(X2), Fly Ash(X3), Water(X4), Superplasticizer(X5), and Age(X8).

#Questions of Interest

For my first data set, real estate in Taiwan, I wanted to see if the price of the home was dependent on the age of the house, distance to metro stations, number of convenience stores, as well as the degree of latitude and longitude that the house was located at. my first step is to test to see whether or not all of these predictors had any significant effect on the price of the house. Once I have found the best fit to predict the price of the house, I can look at my model to see which predictors cause an increase in housing price or a decrease in it. On top of this I can answer the mean price for a house given it is a certain distance from a metro, is a certain age, is located on the West coast ... etc. For my second data set concrete, I wanted to see if the concrete compressive strength was dependent on its makeup. Thus, once I have found the best components of concrete that predict its compressive strength, I am able to see which components increase or decrease its strength. Furthermore, given a certain makeup of the concrete, I will be able to see the mean compressive strength as well as predict the compressive strength of all concrete of the same makeup. 

#Regression Methods

To answer my questions of interest, for part 1 with the real estate data set, in order to fit my model I first looked at a scatterplot matrix in order to determine relationships between the predictors and the response, as well as relationships between the predictors themselves. Next, I fit a base model and tested whether or not I should add the variable Metro and/or Longitude. Once I had found my best model, the next step was to determine if the model needed to have its predictors and response transformed. I did this by doing power transform tests for the predictors and Box Cox estimations for the response. For part 2 with the concrete data set, in order to fit my model I used both forward and backward selection with BIC criterion. Once I found my best model I went on to run diagnostics to check if my model assumptions held. To do this I looked at added variable plots to test relations, residual vs. fitted plots to test linearity, scale-invariance plot to test constant variance, as well as Q-Q plot to test normality. 

#Regression Analysis

#Part 1:

Here I will be looking at the Real Estate Valuation Data set:

```{r}
RealEstate <- read.table('RealEstateValuation.txt')
Price <- RealEstate$Price
TDate <- RealEstate$TDate
Age <- RealEstate$Age
Stores <- RealEstate$Stores
Latitude <- RealEstate$Latitude
Metro <- RealEstate$Metro
Longitude <- RealEstate$Longitude
```

##Checking Relationships Between Variables

The first step I will take is to explore any relationships between the predictors and response, or between the predictors using the model Price ~ TDate + Age + Stores + Latitude. Based on intuition, I would expect to see a relationship between Price and Age, as well as Price and Stores. However, I do not expect to see any significant relationships between the predictors TDate, Age, Stores, and Latitude. Now, looking at the scatter plot matrix to see any relationships:

```{r}
scatterplotMatrix(~Price + TDate + Age + Stores + Latitude)
```

Here I can see that Price vs. each predictor seems to have some sort of relationship as expected. Looking at the plots comparing my predictors to each other, I see that the relation between TDate and Age, Stores, or Latitude seems to have no significant relationship; similarly between Age and Stores there seems to be no correlation. Now looking at the relationship between Stores and Latitude, it looks like a normal distribution when Latitude is the predictor and Stores is the response, and a flat line when Stores is the predictor and Latitude is the response. Thus, it seems that given a certain latitude, I may expect there to be more stores around.  


##Fitting a Model to Predict Price

Now I will run a regression to see if I can predict house price using the predictors TDate, Age, Stores, and Latitude. 

```{r}
fit1 <- lm(Price ~ TDate + Age + Stores + Latitude)
summary(fit1)
```

Using the summary I can see that the model for my data is Price = -1.742e+04(TDate) - 3.02e-1(Age) + 1.929(Stores) + 4.078e+02(Latitude). Also, I are given all of the P-values of the individual coefficients. I can see that all coefficients, expect for TDate's main effect are significant at the alpha = 0.01 level. 

Looking at all of the significant coefficients at alpha = 0.01 level, Age, Stores, and Latitude, I find that for every year the house ages, the price will drop 3,020 New Taiwan Dollars/Ping. For every store added to the living circle on foot, the price of the house will rise 19,200 New Taiwan Dollars/Ping. Lastly, for every degree of latitude added, the price of the house will rise 4,078,00 New Taiwan Dollars/Ping.


##Testing To add Metro and/or Longitude

Now I will perform partial F-Tests to test out if Metro and Longitude should be added to the model. 

H0:The coefficient for Longitude is 0 vs. Ha:The coefficient for Longitude is not 0
at level alpha = 0.05

```{r}
fit2 <- lm(Price ~ TDate + Age + Stores + Latitude + Longitude)
summary(fit2)
```

Thus based off of my T-test, I find my test statistic = 6.033, with a null distribution of T_n-p-1 = T_408. my P-value is 3.60e-09 which is less than my alpha = 0.05; thus, I reject my null hypothesis. 


H0:The coefficient for Metro is 0 vs. Ha:The coefficient for Metro is not 0
at level alpha = 0.05

```{r}
fit3 <- lm(Price ~ TDate + Age + Stores + Latitude + Metro)
summary(fit3)
```

Thus based off of my T-test, I find my test statistic = -8.887, with a null distribution of T_n-p-1 = T_408. my P-value is 2e-16 which is less than my alpha = 0.05; thus, I reject my null hypothesis. 


H0:The coefficient for longitude is 0 with metro in my model vs. Ha: The coefficient for longitude is not 0 with metro is in my model
at level alpha = 0.05

```{r}
fit4 <- lm(Price ~ TDate + Age + Stores + Latitude + Metro + Longitude)
summary(fit4)
```

Thus based on my T-test, I find my test statistic = -0.256, with a null distribution of T_n-p-1 = T_408. my P-value is 0.79829 which is not less than my alpha = 0.05; thus, I fail to reject my null hypothesis. 

Since I have found that the main effect for Metro is significant both with and without Longitude in the model, whereas Longitude is only significant when Metro is not in the model, I have determined my best model is Price ~ TDate + TDate + Age + Latitude + Metro. 

##Considering Another Model

Here I will compare two different models, Price ~ TDate + Age + Metro + Latitude and Price ~ TDate + Age + Stores + Latitude

```{r}
fit5 <- lm(Price ~ TDate + Age + Metro + Latitude)
summary(fit1)
summary(fit5)
```

Based on my summary table I can see that fit1 has a correlation coefficient of 0.5015 whereas fit5 has a correlation coefficient of 0.5448. Since my model fit5 (Price ~ TDate + Age + Metro + Latitude) explains more of the variance in house price given its predictors, I will use this model. On top of this, I see that the main effect of TDate is also significant at a high level in fit5 than in fit1.  

###Transforming the Predictors

Since the predictor Age contains values that are 0, and I cannot power transform or log transform these predictors. Therefore, I must do some manipulation to make sure my predictors are strictly non-zero. 

```{r}
RealEstate$Age <- with(RealEstate, (Age + 0.01))
```

Now I test to see if I need to power transform:

```{r}
RE.pt <- powerTransform(cbind(TDate, Age, Metro, Latitude) ~ 1, RealEstate)
summary(RE.pt)
```

Here I can see all predictors except for Age and Metro contain 1, thus I must look to see what transformation of age and Metro I need. 

```{r}
testTransform(RE.pt, lambda = c(1, 0.5, 0, 1))
```

Thus if I take the square root of Age and a log transformation of Metro, 

```{r}
sqrt_Age <- sqrt(Age)
RE_trsf = with(RealEstate, data.frame(Price, TDate, sqrt(Age), log(Metro), Latitude))
pairs(RE_trsf)
```

Now I can see linear relationships of all of the predictors with the response. 

###Transforming the Response

Now that I have found transformations for my predictors, I will look to see if I need to transform my response. 

```{r}
boxCox(fit5)
```

Since I see that 0 is within my interval, I choose lambda = 0 and log transform my response Price. Hence, my model 
looks like log(Price) ~ TDate + sqrt(Age) + log(Metro) + Latitude.

```{r}
final.fit <- lm(log(Price) ~ TDate  + sqrt(Age) + log(Metro) + Latitude)
summary(fit5)
summary(final.fit)
```

Comparing my fit from before my transformation on Price, Age, and Metro and after, I can see a large increase in my correlation coefficient. Before it was 0.5448, and after taking log(Price), sqrt(Age), and log(Metro), it is 0.7218. Hence I saw a large improvement in my model after doing my transformations. 

##Summary

Based off of my regressions, I have found that, within my data set, the active regressors to calculate the expected log(price) of a house in Sindian District, New Taipei City, Taiwan, are best predicted by TDate, sqrt(Age), log(Metro), and Latitude. Some interesting results that I found in fitting my model was the fact that longitude was not significant in my model, given that metro. This could be because when you account for metro stations in the area, given the layout of Taiwan, on the Western shore, there are a lot of cities/urban areas that span the entire length of the country North to South. Since this entire Urban area is connected and runs longitude wise of the country, once I account for availability in transportation by metro in these areas, difference in longitude becomes less of an issue. Therefore, since movement is accounted for, the difference in price does not seem to be determined by longitude location. 


#Part 2:

Here I will be looking at the data set 'Concrete' consisting of 8 predictors X1(Cement), X2(Blast Furnace Slag), X3(Fly Ash), X4(Water), X5(Superplasticizer), X6(Coarse Aggregate), X7(Fine Aggregate), X8(Age) and 1 response Y(Concrete Compressive Strength). 

```{r}
Concrete <- read.table('Concrete.txt')
X1 <- Concrete$X1
X2 <- Concrete$X2
X3 <- Concrete$X3
X4 <- Concrete$X4
X5 <- Concrete$X5
X6 <- Concrete$X6
X7 <- Concrete$X7
X8 <- Concrete$X8
Y <- Concrete$Y
```

##Forward Model Selection

To find my model I will use forward selection using BIC as a criterion function: 

```{r}
m0 <- lm(Y ~ 1)
full <- ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8
n <- length(Y)

fit6 <- step(m0, full, direction = 'forward', k = log(n))
summary(fit6)
```

The result of forwards BIC model selection is the model: Y ~ X1 + X5 + X8 + X2 + X4 + X3. 

##Diagnostic Checks

```{r}
scatterplotMatrix(~ Y + X1 + X5 + X8 + X2 + X4 + X3)
```

Here it seems that the predictors all have a linear relationship with the response variable. Hence, I will continue with diagnostics to make sure my model assumptions hold.

###Added Variable Plot

```{r}
avPlots(fit6)
```


The marginal relationships are shown in the Added Variable plots: the plots regarding X1, X3, X2 seem like better linear fits, whilst the plot regarding X5 has a good looking linear, it may not tell us anything useful because the fit is around the the line Y|Others = 0.  The plots regarding X4 and X8 seem to have a quadratic curve to them.  I may or may not take a closer look at this later. 



Now in order to test whole model assumptions, I will be looking at a Residual vs. Fitted plot to test linearity, scale-location to test constant variance, and QQ plots for normality of my data. 

###Residual Vs. Fitted for Linearity

```{r}
plot(fit6, which = 1)
```


From the Residual Vs. Fitted Values, I observe some clustering on the left side which leads to the conclusion that this does violate the linearity assumption. 


###Scale-Location for constant variance

```{r}
plot(fit6, which = 3)
```

Again I see some clustering on the left, thus I may see a violation of constant variance in my model. To test to see if there is an effect of mean on the variance I will run a non-constant variance test.

```{r}
ncvTest(fit6)
```

Thus, since I see a significant P-Value, I determine that variance increases as a function of the fitted values. Therefore in order to fix this issue of non-constant variance, I would have to put weights on the fitted values as 1/fitted values.  

###Q-Q Plot to Test Normality

```{r}
plot(fit6, which = 2)
```

It looks like observations 225, 384, and 382 may cause this to be a slightly heavy-tailed plot.  Nonetheless, it violates the normality assumption since the plot is right-skewed.

##Finding Influential Points

```{r}
influenceIndexPlot(fit6, vars = c('hat', 'cook'))
```

Here I see the data point 57 has a high cooks distance as well as a high hat value/leverage, thus it is the most influential point. Similarly point 611 has a large cooks distance. Since these points have high influence, I may consider either adding predictors in my model to account for these points, or even removing them altogether.

##Estimating the Mean Response

Suppose that I want to predict the mean compressive strength of concrete with the makeup of 149kg per meter cubed of cement, 118kg per meter cubed of blast furnace slag, 92kg per meter cubed of fly ash, 183kg per meter cubed of water, 7kg per meter cubed of superplasticizer, and 28 days old. 

```{r}
x.new <- data.frame(X1 = 149, X2 = 118, X3 = 92, X4 = 183, X5 = 7, X8 = 28)
predict(fit6, newdata = x.new, interval = "confidence", level = 0.95)
```

I see an estimated mean concrete compressive strength of 26.17 MPa with a lower bound of 25.12 MPa and an upper bound of 27.22 MPa with 95% confidence.  

##Predicting New Responses

Now looking at the same concrete as above, I want to predict that compressive strength that this makeup gives us for all concrete of this makeup. 

```{r}
predict(fit6, newdata = x.new, interval = "predict", level = 0.95)
```

Thus I can predict that the compressive strength for all concrete with this makeup will lie between 5.72Mpa and 46.62 MPa with 95% confidence. 


##Backward Model Selection

Now I will apply the backward model selection using the BIC criterion. I start with my full model then compare to see if a submodel would be a better fit.

```{r}
m1<-update(m0,full)

fit7 <- step(m1, scope = c(lower= ~X1), direction = 'backward', k = log(n))
```

The result of the backwards BIC model selection is the model: Y ~ X1 + X5 + X8 + X2 + X4 + X3.  Through the backwards selection process, I have removed the predictors "Course Aggregate" (X6) and "Fine Aggregate" (X7) and have found that a smaller model is a better fit with lower BIC.  

##Diagnostic Checks

First, I will look at the scatterplot matrix for my model to determine whether it is reasonable to use linear regression to analyze the data.

```{r}
scatterplotMatrix(~ Y + X1 + X5 + X8 + X2 + X4 + X3)
```

The scatterplot matrix shows that all the predictors have some sort of relationship with Cement Compressive Strength (MPa), therefore, it is reasonable to analyze this data with regression. I will continue to run diagnostics on my model by looking at the Added Variable Plots.

```{r}
avPlots(fit7)
```

I can see marginal relationships from the added variable plots for each predictor, given all others are held constant.  Clearly, they all seem to have a linear relationship of some sort.  However, let it be noted that Y~X8|others and Y~X4|others appear to have a slight quadratic curve to it.  The slopes of these graphs gives the estimated coefficient for their respective predictors (given all others are held constant).    


I will continue to run diagnostics by using residual plots to check my assumptions of the model.  
First, I will look at my Residual vs. Fitted plot to assess my assumption of linearity.

```{r}
plot(fit7, which = 1)
```

The Residual vs. Fitted plot shows slight clustering on the left which leads us to believe that it is not as linear as I would like it to be.

Next, I will look at a QQ-plot to assess my assumption of normality.

```{r}
plot(fit7, which = 2)
```

There does not seem to be any heavy tails in my QQ plot but the plot appears to be right-skewed.  This violates my assumption of normality.

Finally, I will take a look at a Scale-Location plot to determine constant variance.

```{r}
plot(fit7, which = 3)
```

The scale-location plot also shows slightly more clustering on the left than right and seems to slowly spread out from left to right; this violates the constant variance assumption.

##Finding Influential Points:

I will look at influence by examining the hat-values and Cook's distance:

```{r}
influenceIndexPlot(fit7, vars = c('hat', 'Cook'))
```

From the two diagnostic plots, I can see that observation 57 has large leverage and Cook's distance, thus it is the most influential point.  Likewise, Observation 611 also has a fairly high Cook's distance.  Because these points are significantly influential, I may want to consider adding predictor(s).  I may even want to consider removing those observations.  

When using BIC as the criterion function, both the forward selection and backwards elimination algorithm yielded the same model.  


##Summary

I took a model using BIC criterion and applied both forwards and backwards selection algorithms to find the best model for predicting Concrete Compressive Strength (MPa). After finding the best model, Y ~ X1 + X5 + X8 + X2 + X4 + X3, which was the same for forwards and backwards selection (but had different ordering), I checked linearity, constant variance, and normality with residual and QQ plots.  I found some leverages and influences and decided what data to remove to better the model.  I then calculated a confidence and prediction interval which I interpreted directly after.


#Conclusion

In summary, for the real estate data set, the best model fit I found was log(Price) = -6.327e02 + 1.794e-01(TDate) – 4.78e-02(sqrt(Age)) – 2.05e-01(log(Metro)) + 1.108e01(Latitude). Thus, I can see a negative association with the age of the house, meaning that the price of the house declines with age, as expected. However, a more interesting finding is that closeness to a metro station also declines the price of the house. The generalizability of this model may be questionable due to the fact that the price is solely determined by location of the house. This model does not take into account the size or characteristics (number of beds/baths and square footage) of the house. For the concrete data set, the best model fit I found was Y = 29.03 + 0.105(X1) + 0.068(X2) + 0.069(X3) - 0.218(X4) + 0.239(X5) + 0.113(X8). Thus I see that concrete compressive strength decreases for every 1kg per meter cubed of water. Therefore, for every kg per meter cubed of cement, blast furnace slag, fly ash, superplasticizer adds compressive strength, well as the number of days old. The reliability of this model may be questionable because the relationship between concrete compressive strength and the amount of fly ash and blast furnace slag should be negative, however, I found a positive association. 


\centering





```{r, include=FALSE}
save.image('FinalProject.RData')
```


